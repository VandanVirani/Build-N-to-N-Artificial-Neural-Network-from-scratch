## Welcome 

𝐓𝐡𝐢𝐬 𝐫𝐞𝐩𝐨𝐬𝐢𝐭𝐨𝐫𝐲 𝐜𝐨𝐧𝐭𝐚𝐢𝐧𝐬 𝐜𝐨𝐝𝐞 𝐭𝐨 𝐛𝐮𝐢𝐥𝐝 𝐍 𝐭𝐨 𝐍 𝐚𝐫𝐭𝐢𝐟𝐢𝐜𝐢𝐚𝐥 𝐍𝐞𝐮𝐫𝐚𝐥 𝐍𝐞𝐭𝐰𝐨𝐫𝐤 , 𝐍 𝐭𝐨 𝐍 𝐦𝐞𝐚𝐧𝐬 𝐍 𝐧𝐮𝐦𝐛𝐞𝐫 𝐨𝐟 𝐥𝐚𝐲𝐞𝐫𝐬 𝐚𝐧𝐝 𝐮𝐧𝐢𝐭𝐬 . 
𝐖𝐞 𝐚𝐫𝐞 𝐠𝐨𝐢𝐧𝐠 𝐭𝐨 𝐮𝐬𝐞 𝐭𝐡𝐞 𝐜𝐨𝐧𝐜𝐞𝐩𝐭 𝐨𝐟 𝐨𝐨𝐩 𝐞.𝐠 𝐜𝐥𝐚𝐬𝐬 , 𝐢𝐧𝐬𝐭𝐚𝐧𝐜𝐞 . 𝐓𝐡𝐢𝐬 𝐩𝐫𝐨𝐣𝐞𝐜𝐭 𝐫𝐞𝐪𝐮𝐢𝐫𝐞 𝐤𝐧𝐨𝐰𝐥𝐞𝐝𝐠𝐞 𝐚𝐛𝐨𝐮𝐭 𝐨𝐨𝐩 𝐜𝐨𝐧𝐜𝐞𝐩𝐭 , 𝐟𝐨𝐫 𝐥𝐨𝐨𝐩 , 𝐚𝐫𝐫𝐚𝐲𝐬 , 𝐦𝐚𝐭𝐫𝐢𝐱 , 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 , 𝐧𝐮𝐦𝐩𝐲 , 𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝐚𝐫𝐲 . 
𝐖𝐞 𝐰𝐢𝐥𝐥 𝐛𝐮𝐢𝐥𝐝 𝐨𝐮𝐫 𝐨𝐰𝐧 𝐦𝐨𝐝𝐞𝐥 𝐚𝐧𝐝  𝐰𝐞 𝐰𝐢𝐥𝐥 𝐢𝐦𝐩𝐥𝐞𝐦𝐞𝐧𝐭 𝐌𝐍𝐈𝐒𝐓 𝐝𝐚𝐭𝐚𝐬𝐞𝐭 𝐭𝐨 𝐨𝐮𝐫 𝐦𝐨𝐝𝐞𝐥 . 

MNIST dataset 
<img src="https://user-images.githubusercontent.com/76767487/148059145-c2b13ff0-ac67-4f79-b170-11de64a3d7a6.png" width=600 height=400 />

## 𝐋𝐄𝐓𝐒 𝐆𝐄𝐓 𝐒𝐓𝐀𝐑𝐓𝐄𝐃 

𝐅𝐢𝐫𝐬𝐭  𝐰𝐞 𝐰𝐢𝐥𝐥 𝐮𝐬𝐞 𝐭𝐡𝐢𝐬 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧  𝐭𝐨 𝐠𝐞𝐭 𝐭𝐡𝐞 𝐢𝐧𝐩𝐮𝐭 𝐚𝐧𝐝 𝐨𝐮𝐩𝐮𝐭 𝐟𝐫𝐨𝐦 𝐦𝐧𝐢𝐬𝐭 𝐝𝐚𝐭𝐚 . 𝐓𝐡𝐞 𝐌𝐍𝐈𝐒𝐓 𝐝𝐚𝐭𝐚𝐛𝐚𝐬𝐞 (𝐌𝐨𝐝𝐢𝐟𝐢𝐞𝐝 𝐍𝐚𝐭𝐢𝐨𝐧𝐚𝐥 𝐈𝐧𝐬𝐭𝐢𝐭𝐮𝐭𝐞 𝐨𝐟 𝐒𝐭𝐚𝐧𝐝𝐚𝐫𝐝𝐬 𝐚𝐧𝐝 𝐓𝐞𝐜𝐡𝐧𝐨𝐥𝐨𝐠𝐲 𝐝𝐚𝐭𝐚𝐛𝐚𝐬𝐞[𝟏]) 𝐢𝐬 𝐚 𝐥𝐚𝐫𝐠𝐞 𝐝𝐚𝐭𝐚𝐛𝐚𝐬𝐞 𝐨𝐟 𝐡𝐚𝐧𝐝𝐰𝐫𝐢𝐭𝐭𝐞𝐧 𝐝𝐢𝐠𝐢𝐭𝐬 𝐭𝐡𝐚𝐭 𝐢𝐬 𝐜𝐨𝐦𝐦𝐨𝐧𝐥𝐲 𝐮𝐬𝐞𝐝 𝐟𝐨𝐫 𝐭𝐫𝐚𝐢𝐧𝐢𝐧𝐠 𝐯𝐚𝐫𝐢𝐨𝐮𝐬 𝐢𝐦𝐚𝐠𝐞 𝐩𝐫𝐨𝐜𝐞𝐬𝐬𝐢𝐧𝐠 𝐬𝐲𝐬𝐭𝐞𝐦𝐬 . 𝐓𝐡𝐞 𝐌𝐍𝐈𝐒𝐓 𝐝𝐚𝐭𝐚𝐛𝐚𝐬𝐞 𝐜𝐨𝐧𝐭𝐚𝐢𝐧𝐬 𝟔𝟎,𝟎𝟎𝟎 𝐭𝐫𝐚𝐢𝐧𝐢𝐧𝐠 𝐢𝐦𝐚𝐠𝐞𝐬 . 
𝐁𝐞𝐜𝐚𝐮𝐬𝐞 𝐭𝐡𝐞𝐫𝐞 𝐚𝐫𝐞 𝐭𝐨𝐭𝐚𝐥 𝟏𝟎 𝐝𝐢𝐠𝐢𝐭𝐬 𝟎,𝟏,𝟐,𝟑,𝟒,𝟓,𝟔,𝟕,𝟖,𝟗 𝐭𝐡𝐞𝐫𝐞 𝐰𝐢𝐥𝐥 𝐛𝐞 𝟏𝟎 𝐜𝐥𝐚𝐬𝐬 𝐢𝐧 𝐥𝐚𝐬𝐭 𝐥𝐚𝐲𝐞𝐫 .
```yml 
import  numpy as np

def get_mnist():
    with np.load(f"D:/python programing/test/ann/mnist.npz") as f:
        images, labels = f["x_train"], f["y_train"]
    images = images.astype("float32") / 255
    images = np.reshape(images, (images.shape[0], images.shape[1] * images.shape[2]))
    labels = np.eye(10)[labels]
    return images, labels
input,output= get_mnist()
print(input,output)
print(input.shape,output.shape)
```
<img src="https://user-images.githubusercontent.com/76767487/148325194-287d827c-a3dd-4fda-9038-cb1349feba94.jpeg" width=900 height=600 />

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
𝐰𝐞 𝐰𝐢𝐥𝐥 𝐜𝐫𝐞𝐚𝐭𝐞 𝐜𝐥𝐚𝐬𝐬 𝐰𝐡𝐢𝐜𝐡 𝐰𝐢𝐥𝐥 𝐡𝐞𝐥𝐩 𝐭𝐨 𝐠𝐞𝐭 𝐢𝐧𝐬𝐭𝐚𝐧𝐜𝐞 , 𝐚𝐧𝐝 𝐢𝐧𝐬𝐢𝐝𝐞 𝐭𝐡𝐚𝐭 𝐟𝐢𝐫𝐬𝐭 𝐰𝐞 𝐰𝐢𝐥𝐥 𝐢𝐧𝐢𝐭𝐢𝐚𝐥𝐢𝐳𝐞 𝐥𝐢𝐬𝐭 𝐚𝐧𝐝 𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝐚𝐫𝐲 , 𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝐚𝐫𝐲 𝐢𝐬 𝐦𝐚𝐢𝐧 𝐬𝐭𝐨𝐫𝐚𝐠𝐞 𝐭𝐨 𝐬𝐭𝐨𝐫𝐞 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 , 𝐮𝐧𝐢𝐭𝐬 .
```yml
 class ANN : 
    def __init__(self):
        self.no_of_units_in_layers=[]
        self.activation_fun=[]
        self.weights={}
        self.units={}
        self.bias={}
```
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
𝐁𝐞𝐥𝐨𝐰  ,  𝐰𝐞 𝐡𝐚𝐯𝐞 𝐦𝐚𝐝𝐞 𝐢𝐧𝐬𝐭𝐚𝐧𝐜𝐞 𝐨𝐟 𝐀𝐍𝐍 𝐜𝐥𝐚𝐬𝐬 𝐰𝐡𝐢𝐜𝐡 𝐢𝐬 𝐱 , 𝐡𝐞𝐫𝐞 𝐰𝐞 𝐡𝐚𝐯𝐞 𝐦𝐚𝐝𝐞 𝐚 𝐚𝐝𝐝 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐰𝐡𝐢𝐜𝐡 𝐰𝐢𝐥𝐥 𝐜𝐫𝐞𝐚𝐭𝐞 𝐚 𝐥𝐚𝐲𝐞𝐫 , 𝐢𝐭 𝐫𝐞𝐪𝐮𝐢𝐫𝐞 𝐧𝐨 𝐨𝐟 𝐮𝐧𝐢𝐭𝐬 𝐚𝐧𝐝 𝐚𝐧 𝐚𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐚𝐬𝐬𝐨𝐜𝐢𝐚𝐭𝐞𝐝 𝐭𝐨 𝐭𝐡𝐚𝐭 𝐥𝐚𝐲𝐞𝐫 . 𝐰𝐞 𝐡𝐚𝐯𝐞 𝐚𝐝𝐝𝐞𝐝 𝐭𝐡𝐞 𝐯𝐚𝐥𝐮𝐞 𝐨𝐟 𝐮𝐧𝐢𝐭𝐬 𝐚𝐧𝐝 𝐚𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧 𝐭𝐨 𝐥𝐢𝐬𝐭 . 𝐡𝐞𝐫𝐞 𝐰𝐞 𝐡𝐚𝐯𝐞 𝐚𝐝𝐝 𝐭𝐰𝐨 𝐭𝐢𝐦𝐞 𝐚𝐝𝐝 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐦𝐞𝐚𝐧𝐬 𝐰𝐞 𝐡𝐚𝐯𝐞 𝐭𝐰𝐨 𝐥𝐚𝐲𝐞𝐫𝐬 𝐨𝐧𝐞 𝐰𝐢𝐭𝐡 𝟑 𝐮𝐧𝐢𝐭𝐬 𝐰𝐢𝐭𝐡 𝐬𝐢𝐠𝐦𝐨𝐢𝐝 𝐚𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧 𝐚𝐧𝐝 𝐬𝐞𝐜𝐨𝐧𝐝 𝐢𝐬 𝟐 𝐮𝐧𝐢𝐭𝐬 . 𝐥𝐚𝐬𝐭 𝐭𝐢𝐦𝐞 𝐰𝐞 𝐜𝐚𝐥𝐥 𝐚𝐝𝐝 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐢𝐬 𝐨𝐮𝐫 𝐨𝐮𝐭𝐩𝐮𝐭 𝐥𝐚𝐲𝐞𝐫 , 𝐰𝐞 𝐡𝐚𝐯𝐞 𝟐 𝐮𝐧𝐢𝐭𝐬 𝐢𝐧 𝐨𝐮𝐭𝐩𝐮𝐭 𝐥𝐚𝐲𝐞𝐫 𝐦𝐞𝐚𝐧𝐬 𝐰𝐞 𝐡𝐚𝐯𝐞 𝟐 𝐜𝐥𝐚𝐬𝐬 (:. 𝐥𝐢𝐤𝐞 𝐜𝐚𝐭,𝐝𝐨𝐠 ).
```yml
class ANN:
    def __init__(self):
        self.no_of_units_in_layers=[]
        self.activation_fun=[]
        self.weights={}
        self.units={}
        self.bias={}
    
    def add(self,unit,activation=0):                 # default value of activation is 0
        self.no_of_units_in_layers.append(int(unit))   # append the information in no_of_units_in_layers list
        self.activation_fun.append(activation)         # append the information in activation_fun list

x=ANN()
x.add(3,"sigmoid")
x.add(10)   # because 10 class are there 
```        
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
𝐍𝐨𝐰 𝐢𝐭𝐬 𝐭𝐢𝐦𝐞 𝐭𝐨 𝐜𝐫𝐞𝐚𝐭𝐞 𝐚𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 , 𝐢𝐭 𝐭𝐚𝐤𝐞𝐬 𝐭𝐰𝐨 𝐚𝐫𝐠𝐮𝐦𝐞𝐧𝐭𝐬 𝐟𝐢𝐫𝐬𝐭 𝐢𝐬 𝐚𝐫𝐫𝐚𝐲 𝐚𝐧𝐝 𝐬𝐞𝐜𝐨𝐧𝐝 𝐢𝐬 𝐚 𝐧𝐮𝐦𝐛𝐞𝐫 𝐢𝐭 𝐭𝐞𝐥𝐥𝐬 𝐰𝐡𝐚𝐭 𝐥𝐚𝐲𝐞𝐫 𝐧𝐞𝐞𝐝 𝐭𝐨 𝐮𝐬𝐞 𝐰𝐡𝐢𝐜𝐡 𝐚𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 . 
𝐬𝐢𝐠𝐦𝐨𝐢𝐝 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 : 𝐜𝐨𝐧𝐯𝐞𝐫𝐭𝐬 +𝐢𝐯𝐞 𝐢𝐧𝐭𝐞𝐠𝐞𝐫 𝐭𝐨 𝐚  𝐧𝐮𝐦𝐛𝐞𝐫 𝐢𝐧 𝐛𝐞𝐭𝐰𝐞𝐞𝐧 𝟎 𝐭𝐨 𝟏 . 𝐟𝐨𝐫𝐦𝐮𝐥𝐚 𝐢𝐬 𝟏 / (𝟏 + 𝐞**(-𝐱) ) , 𝐰𝐞 𝐡𝐚𝐯𝐞 𝐮𝐬𝐞 𝐦𝐚𝐭𝐡 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐭𝐨 𝐮𝐬𝐞 𝐯𝐚𝐥𝐮𝐞 𝐨𝐟 𝐞 . 
𝐫𝐞𝐥𝐮 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧    : 𝐢𝐟 𝐯𝐚𝐥𝐮𝐞 𝐥𝐞𝐬𝐬 𝐭𝐡𝐚𝐧 𝟎 𝐭𝐡𝐚𝐧 𝐢𝐭 𝐰𝐢𝐥𝐥 𝐫𝐞𝐭𝐮𝐫𝐧 𝟎 𝐨𝐭𝐡𝐞𝐫𝐰𝐢𝐬𝐞 𝐢𝐭 𝐰𝐢𝐥𝐥 𝐫𝐞𝐭𝐮𝐫𝐧 𝐭𝐡𝐞 𝐨𝐫𝐢𝐠𝐢𝐧𝐚𝐥 𝐯𝐚𝐥𝐮𝐞 .
```yml
class ANN:
    import math
    def __init__(self):
        self.no_of_units_in_layers=[]
        self.activation_fun=[]
        self.weights={}
        self.units={}
        self.bias={}
    
    def add(self,unit=0,activation=0):  # default value of activation is 0
        self.no_of_units_in_layers.append(int(unit))
        self.activation_fun.append(activation)
    def activation_function(self,x,i):
        if self.activation_fun[i]=="sigmoid":
            return 1/(1 + self.math.e**(-x))
        elif self.activation_fun[i]=="relu":
            return x*(x>0)    
x=ANN()
x.add(3,"sigmoid")
x.add(2)
```     
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
𝐍𝐨𝐰 𝐢𝐭𝐬 𝐭𝐢𝐦𝐞 𝐭𝐨 𝐭𝐚𝐤𝐞 𝐢𝐧𝐩𝐮𝐭 , 𝐨𝐮𝐭𝐩𝐮𝐭 , 𝐥𝐞𝐚𝐫𝐧𝐢𝐧𝐠 𝐫𝐚𝐭𝐞 , 𝐞𝐩𝐨𝐜𝐡𝐬 𝐭𝐨 𝐨𝐮𝐫 𝐦𝐨𝐝𝐞𝐥 .𝐭𝐨 𝐝𝐨 𝐭𝐡𝐚𝐭 𝐰𝐞 𝐡𝐚𝐯𝐞 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐜𝐚𝐥𝐥𝐞𝐝 𝐚𝐧𝐧. 𝐚𝐥𝐥 𝐭𝐡𝐞 𝐭𝐡𝐢𝐧𝐠𝐬 𝐡𝐚𝐩𝐩𝐞𝐧 𝐢𝐧𝐬𝐢𝐝𝐞 𝐭𝐡𝐞 𝐚𝐧𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 . 𝐰𝐞 𝐮𝐬𝐞 𝐧𝐮𝐦𝐩𝐲 𝐥𝐢𝐛𝐫𝐚𝐫𝐲 𝐟𝐨𝐫 𝐜𝐫𝐞𝐚𝐭𝐢𝐨𝐧 𝐨𝐟 𝐚𝐫𝐫𝐚𝐲 , 𝐠𝐞𝐭𝐭𝐢𝐧𝐠 𝐫𝐚𝐧𝐝𝐨𝐦 𝐯𝐚𝐥𝐮𝐞𝐬 . 𝐖𝐞 𝐡𝐚𝐯𝐞 𝐜𝐚𝐥𝐥 𝐚𝐧𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐚𝐭 𝐥𝐚𝐬𝐭 𝐢𝐧 𝐜𝐨𝐝𝐞. 𝐢𝐧𝐩𝐮𝐭 𝐢𝐬 𝐚𝐧 𝐚𝐫𝐫𝐚𝐲 𝐟𝐨𝐫 𝐞𝐱𝐚𝐩𝐥𝐞 𝐌𝐍𝐈𝐒𝐓 𝐝𝐚𝐭𝐚𝐬𝐞𝐭 𝐜𝐨𝐧𝐭𝐚𝐢𝐧𝐬 
𝟏) 𝐜𝐫𝐞𝐚𝐭𝐢𝐨𝐧 𝐨𝐟 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 : 𝐢𝐧𝐢𝐭𝐢𝐚𝐥𝐥𝐲 𝐭𝐡𝐞 𝐯𝐚𝐥𝐮𝐞 𝐨𝐟 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 𝐢𝐬 𝐫𝐚𝐧𝐝𝐨𝐦 𝐚𝐧𝐝 𝐠𝐞𝐭 𝐜𝐡𝐚𝐧𝐠𝐞 𝐛𝐲 𝐛𝐚𝐜𝐤𝐩𝐫𝐨𝐩𝐚𝐠𝐚𝐭𝐢𝐨𝐧 . 𝐭𝐨 𝐬𝐭𝐨𝐫𝐞 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 𝐰𝐞 𝐮𝐬𝐞 𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝐚𝐫𝐲

```yml

class ANN:
    import math,numpy as np
    def __init__(self):
        self.no_of_units_in_layers=[]
        self.activation_fun=[]
        self.weights={}
        self.units={}
        self.bias={}
    
    def add(self,unit=0,activation=0):  # default value of activation is 0
        self.no_of_units_in_layers.append(int(unit))
        self.activation_fun.append(activation)
    def activation_function(self,x,i):
        if self.activation_fun[i]=="sigmoid":
            return 1/(1 + self.math.e**(-x))
        elif self.activation_fun[i]=="relu":
            return x*(x>0)    
    def ann(self,inputs,outputs,epochs=10,learning_rate=0.1):  # default value  of learning rate is 0.1 and epochs is 10 
       ###### creation of weights 
       self.weights['0']=np.random.uniform(-0.5,0.5,(len(inputs[0]),self.no_of_units_in_layers[0]))
       for i in range(len(self.no_of_units_in_layers)-1):
           self.weights['{}'.format(i+1)]=np.random.uniform(-0.5,0.5,(self.no_of_units_in_layers[i],self.no_of_units_in_layers[i+1]))
       print("weights : ",self.weights)    
       ###### initializing the units 
       for i in range(len(self.no_of_units_in_layers)):
           self.units['{}'.format(i+1)] = np.zeros(self.no_of_units_in_layers[i])
       print("units : ",self.units)        
       for i in range(len(self.units)):
           self.bias['{}'.format(i+1)]=np.zeros(len(self.units['{}'.format(i+1)]))
       print("bias : ",self.bias)    
       
x=ANN()
x.add(3,"sigmoid")
x.add(10)
x.ann(input,output,5,learning_rate=0.01)    # input and output are taken from mnist dataset . 
```  

<img src="https://user-images.githubusercontent.com/76767487/148326714-1543568b-bd7d-4b36-8e71-06b12e3107c9.jpg" width=900 height=230 />

𝐍𝐨𝐰 𝐰𝐞 𝐰𝐢𝐥𝐥 𝐛𝐞 𝐭𝐡𝐢𝐧𝐤𝐢𝐧𝐠 𝐡𝐨𝐰 𝐢 𝐡𝐚𝐯𝐞 𝐜𝐫𝐞𝐚𝐭𝐞𝐝 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 . 𝐬𝐞𝐞 𝐛𝐞𝐥𝐨𝐰 𝐢𝐦𝐚𝐠𝐞



![WhatsApp Image 2022-01-06 at 9 59 03 AM](https://user-images.githubusercontent.com/76767487/148328318-cb6342a4-db48-4b8a-9962-e4ba904fb66e.jpeg)

𝐈𝐧𝐢𝐭𝐢𝐚𝐥𝐥𝐲 𝐭𝐡𝐞 𝐮𝐧𝐢𝐭𝐬 𝐯𝐚𝐥𝐮𝐞 𝐢𝐧 𝐮𝐧𝐢𝐭𝐬 𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝐚𝐫𝐲 𝐢𝐬 𝟎 , 𝐢𝐭 𝐰𝐢𝐥𝐥 𝐜𝐡𝐚𝐧𝐠𝐞𝐝 𝐝𝐮𝐫𝐢𝐧𝐠 𝐟𝐨𝐫𝐰𝐚𝐫𝐝 𝐩𝐫𝐨𝐩𝐚𝐠𝐚𝐭𝐢𝐨𝐧 .

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 

## FORWARD PROPAGATION  


```yml
def ann(self,inputs,outputs,epochs=10,learning_rate=0.1):  # default value  of learning rate is 0.1 and epochs is 10 
       ###### creation of weights 
       self.weights['0']=np.random.uniform(-0.5,0.5,(len(inputs[0]),self.no_of_units_in_layers[0]))
       for i in range(len(self.no_of_units_in_layers)-1):
           self.weights['{}'.format(i+1)]=np.random.uniform(-0.5,0.5,(self.no_of_units_in_layers[i],self.no_of_units_in_layers[i+1]))
       print("weights : ",self.weights)    
       ###### initializing the units 
       for i in range(len(self.no_of_units_in_layers)):
           self.units['{}'.format(i+1)] = np.zeros(self.no_of_units_in_layers[i])
       print("units : ",self.units)        
       for i in range(len(self.units)):
           self.bias['{}'.format(i+1)]=np.zeros(len(self.units['{}'.format(i+1)]))
       print("bias : ",self.bias)    
       
       #-------------------------------------------------------->
       for y in range(epochs):
         for e in range(len(outputs)):
            self.units['0']=input[e]
            self.units['real']=outputs[e]
            
            ######   forward phase 
            ########## change one  dimentional array in unit_dict in to two dimensional
            self.units['{}'.format(0)]=np.reshape(self.units['{}'.format(0)],(1,-1))   # because we want to calculate matrix multiplication 
            def forward(units,weights,bias):
                for i in range(len(units)-2):
                    units['{}'.format(i+1)] = self.np.reshape(self.sigmoid(self.np.dot(units['{}'.format(i)] ,weights['{}'.format(i)] )+bias['{}'.format(i+1)],i) ,(-1,))        
            forward(self.units,self.weights,self.bias)
            ########## change two dimentional array in unit_dict in to one dimensional
            self.units['{}'.format(0)]= self.np.reshape(self.units['{}'.format(0)],(-1,))
```  

𝐡𝐞𝐫𝐞 , 𝐭𝐡𝐞 𝐜𝐫𝐞𝐚𝐭𝐢𝐨𝐧 𝐨𝐟 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 , 𝐛𝐢𝐚𝐬 , 𝐢𝐧𝐢𝐭𝐢𝐚𝐥𝐢𝐳𝐢𝐧𝐠 𝐮𝐧𝐢𝐭𝐬 𝐰𝐢𝐥𝐥 𝐫𝐮𝐧 𝐨𝐧𝐥𝐲 𝐨𝐧𝐞 𝐭𝐢𝐦𝐞 𝐛𝐮𝐭 𝐭𝐡𝐞 𝐟𝐨𝐫𝐰𝐚𝐫𝐝 𝐩𝐫𝐨𝐩𝐚𝐠𝐚𝐭𝐢𝐨𝐧 , 𝐛𝐚𝐜𝐤𝐰𝐚𝐜𝐤𝐩𝐫𝐚𝐩𝐚𝐠𝐚𝐭𝐢𝐨𝐧 𝐰𝐢𝐥𝐥 𝐫𝐮𝐧 𝐦𝐨𝐫𝐞 𝐭𝐡𝐚𝐧 𝐨𝐧𝐞 𝐟𝐨𝐫 𝐭𝐫𝐚𝐢𝐧𝐢𝐧𝐠 𝐬𝐨 𝐰𝐞 𝐰𝐢𝐥 𝐰𝐫𝐢𝐭𝐞 𝐞𝐧𝐭𝐢𝐫𝐞 𝐟𝐨𝐫𝐰𝐚𝐫𝐝 𝐚𝐧𝐝 𝐛𝐚𝐜𝐤𝐰𝐚𝐫𝐝 𝐜𝐨𝐝𝐞 𝐢𝐧 𝐟𝐨𝐫 𝐥𝐨𝐨𝐩 , 𝐓𝐡𝐞 𝐫𝐞𝐠𝐮𝐥𝐚𝐫𝐢𝐬𝐚𝐭𝐢𝐨𝐧 𝐨𝐟 𝐭𝐡𝐞 𝐟𝐨𝐫 𝐥𝐨𝐨𝐩 𝐰𝐢𝐥𝐥 𝐛𝐞 𝐝𝐨𝐧𝐞 𝐛𝐲 𝐞𝐩𝐨𝐜𝐡𝐬 , 𝐢𝐟 𝐞𝐩𝐨𝐜𝐡𝐬 𝐢𝐬 𝟏 𝐭𝐡𝐞 𝐨𝐮𝐫 𝐜𝐨𝐝𝐞 𝐰𝐢𝐥𝐥 𝐚𝐧𝐚𝐥𝐲𝐬𝐞 𝐭𝐡𝐞 𝐞𝐧𝐭𝐢𝐫𝐞 𝐝𝐚𝐭𝐚𝐬𝐞𝐭 𝐨𝐧𝐜𝐞 , 𝐢𝐟 𝐞𝐩𝐨𝐜𝐡𝐬 𝐢𝐬 𝟏𝟎 𝐭𝐡𝐞𝐧 𝐢𝐭 𝐰𝐢𝐥𝐥 𝐬𝐜𝐚𝐧 𝟏𝟎 𝐭𝐢𝐦𝐞𝐬 . 𝐢𝐟 𝐲𝐨𝐮 𝐝𝐨𝐧'𝐭 𝐮𝐧𝐝𝐞𝐫𝐬𝐭𝐚𝐧𝐝 𝐭𝐡𝐞𝐧 𝐭𝐫𝐲 𝐭𝐨 𝐫𝐮𝐧 𝐭𝐡𝐞 𝐩𝐚𝐫𝐭𝐬 𝐨𝐟 𝐭𝐡𝐞 𝐜𝐨𝐝𝐞 .


----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 
##𝐋𝐎𝐒𝐒 

𝐰𝐞 𝐚𝐫𝐞 𝐚𝐝𝐝𝐢𝐧𝐠 𝐥𝐨𝐬𝐬 𝐜𝐚𝐥𝐜𝐮𝐥𝐚𝐭𝐢𝐨𝐧 , 𝐥𝐨𝐬𝐬 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐜𝐚𝐧 𝐛𝐞 𝐯𝐚𝐫𝐢𝐞𝐬 , 𝐡𝐞𝐫𝐞 𝐰𝐞 𝐮𝐬𝐞𝐝 𝐬𝐪𝐮𝐚𝐫𝐞𝐝 𝐥𝐨𝐬𝐬 𝐢𝐧 𝐚𝐧𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐚𝐟𝐭𝐞𝐫 𝐟𝐨𝐫𝐰𝐚𝐫𝐝 𝐩𝐫𝐨𝐩𝐚𝐠𝐚𝐭𝐢𝐨𝐧 .

```         
            error = (1/len(self.units['real']))* np.sum ((self.units['{}'.format(len(self.units)-2)] - self.units['real'] )**2)
            ######## to check how many data classied correctly in training after how many epochs
            print("    error is  =>  ",error , "   and epochs is   ", y,"  ",e)
```            

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 
## 𝐁𝐀𝐂𝐊𝐏𝐑𝐎𝐏𝐀𝐆𝐀𝐓𝐈𝐎𝐍 

𝐧𝐨𝐰 𝐰𝐞 𝐚𝐫𝐞 𝐮𝐬𝐢𝐧𝐠 𝐚 𝐧𝐞𝐰 𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝐚𝐫𝐲 𝐰𝐡𝐢𝐜𝐡 𝐢𝐬 𝐚 𝐜𝐨𝐩𝐲 𝐨𝐟 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 𝐰𝐡𝐢𝐜𝐡 𝐰𝐢𝐥𝐥 𝐬𝐭𝐨𝐫𝐞 𝐩𝐚𝐫𝐭𝐢𝐚𝐥 𝐝𝐞𝐫𝐢𝐯𝐚𝐭𝐢𝐞𝐬 , 𝐲𝐨𝐮 𝐰𝐢𝐥𝐥  𝐭𝐡𝐢𝐧𝐤 " 𝐰𝐡𝐚𝐭 𝐈 𝐡𝐚𝐯𝐞 𝐝𝐨𝐧𝐞 " . 𝐭𝐡𝐚𝐭 𝐰𝐡𝐲 𝐢𝐭 𝐢𝐬 𝐧𝐞𝐜𝐜𝐞𝐬𝐬𝐚𝐫𝐲 𝐭𝐨 𝐮𝐧𝐝𝐞𝐫𝐬𝐭𝐚𝐧𝐝 𝐭𝐡𝐞 𝐦𝐚𝐭𝐡𝐞𝐦𝐚𝐭𝐢𝐜𝐬 𝐛𝐞𝐡𝐢𝐧𝐝 𝐭𝐡𝐞 𝐛𝐚𝐜𝐤𝐩𝐫𝐨𝐩𝐚𝐠𝐚𝐭𝐢𝐨𝐧
```yml
   def ann(self,inputs,outputs,epochs,learning_rate=0.1):
       ###### creation of weights 
       self.weights['0']=np.random.uniform(-0.5,0.5,(len(inputs[0]),self.no_of_units_in_layers[0]))
       for i in range(len(self.no_of_units_in_layers)-1):
           self.weights['{}'.format(i+1)]=np.random.uniform(-0.5,0.5,(self.no_of_units_in_layers[i],self.no_of_units_in_layers[i+1]))
       for i in range(len(self.no_of_units_in_layers)):
           self.units['{}'.format(i+1)] = np.zeros(self.no_of_units_in_layers[i])
       for i in range(len(self.units)):
           self.bias['{}'.format(i+1)]=np.zeros(len(self.units['{}'.format(i+1)]))
       for y in range(epochs):
         for e in range(len(outputs)):
            self.units['0']=input[e]
            self.units['real']=outputs[e]
            ######   forward phase 
            self.units['{}'.format(0)]=np.reshape(self.units['{}'.format(0)],(1,-1))
            def forward(units,weights,bias):
                for i in range(len(units)-2):
                    units['{}'.format(i+1)] = self.np.reshape(self.sigmoid(self.np.dot(units['{}'.format(i)] ,weights['{}'.format(i)] )+bias['{}'.format(i+1)],i) ,(-1,))        
            forward(self.units,self.weights,self.bias)
            ########## change two dimentional array in unit_dict in to one dimensional
            self.units['{}'.format(0)]= self.np.reshape(self.units['{}'.format(0)],(-1,))
            
            
            ##############  error cost function
            error=(1/len(self.units['real']))* np.sum ((self.units['{}'.format(len(self.units)-2)] - self.units['real'] )**2)
            ######## to check how many data classied correctly in training after how many epochs
            print("    error is  =>  ",error , "   and epochs is   ", y,"  ",e)
            
            
            
            ######## back propagation phase 
            pd_backward_weights=self.copy.deepcopy(self.weights)
            def activ_diff(y):  # we used this function  because of user choise whether the function is sigmoid or relu
                if self.activation_fun[y]=="sigmoid":
                    return self.units['{}'.format(y+1)]*(1-self.units['{}'.format(y+1)])
                elif self.activation_fun[y]=="relu":
                    return self.units['{}'.format(y+1)]*(self.units['{}'.format(y+1)]>0)
                elif self.activation_fun[y]==0:
                    return y       
            def backward(pd_backward_weights):
                #------------------------------------->
                # this is for last layer
                dd = (self.units["{}".format(len(self.units)-2)] - self.units["real"]) * activ_diff(len(self.units)-3) 
                self.bias['{}'.format(len(self.units)-2)] += - learning_rate* dd
                pd_backward_weights['{}'.format(len(self.units)-3)] = dd *self.units['{}'.format(len(self.units)-3)].reshape(-1,1)
                dd = dd* self.weights['{}'.format(len(self.units)-3)]
                self.weights['{}'.format(len(self.units)-3)] += - learning_rate* pd_backward_weights['{}'.format(len(self.units)-3)]
 
                #------------------------------------->
                # this is for all layer except last 
                for i in reversed(range(len(self.weights)-1)):
                   dd = np.sum(dd * activ_diff(i).reshape(-1,1),axis=1)
                   self.bias['{}'.format(i+1)] += - learning_rate*dd
                   pd_backward_weights['{}'.format(i)] = dd * self.units['{}'.format(i)].reshape(-1,1)
                   dd = dd * self.weights['{}'.format(i)]
                   self.weights['{}'.format(i)] += - learning_rate* pd_backward_weights['{}'.format(i)]
            backward(pd_backward_weights)
```

𝐈𝐧 𝐛𝐚𝐜𝐤𝐩𝐫𝐨𝐩𝐚𝐠𝐚𝐭𝐢𝐨𝐧 𝐰𝐞 𝐡𝐚𝐯𝐞 𝐭𝐨 𝐮𝐩𝐚𝐝𝐚𝐭𝐞 𝐛𝐨𝐭𝐡 𝐭𝐡𝐞 𝐛𝐢𝐚𝐬 𝐚𝐧𝐝 𝐰𝐞𝐢𝐠𝐡𝐭 , 𝐭𝐡𝐞𝐢𝐫 𝐭𝐡𝐞𝐫𝐞 𝐚𝐫𝐞 𝐭𝐰𝐨 𝐝𝐢𝐜𝐭 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 𝐚𝐧𝐝 𝐩𝐝_𝐛𝐚𝐜𝐤𝐰𝐚𝐫𝐝_𝐰𝐞𝐢𝐠𝐡𝐭𝐬 𝐰𝐡𝐢𝐜𝐡 𝐢𝐬 𝐚 𝐜𝐨𝐩𝐲 𝐨𝐟 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 , 𝐧𝐨𝐰 𝐰𝐞 𝐰𝐢𝐥𝐥 𝐬𝐭𝐨𝐫𝐞 𝐚𝐥𝐥 𝐭𝐡𝐞 𝐩𝐚𝐫𝐭𝐢𝐚𝐥 𝐝𝐞𝐫𝐢𝐯𝐚𝐭𝐢𝐯𝐞𝐬 𝐢𝐧 𝐩𝐝_𝐛𝐚𝐤𝐜𝐰𝐚𝐫𝐝_𝐰𝐞𝐢𝐠𝐡𝐭𝐬 𝐛𝐞𝐜𝐚𝐮𝐬𝐞 𝐰𝐞 𝐰𝐚𝐧𝐭 𝐭𝐨 𝐬𝐭𝐨𝐫𝐞 𝐧𝐞𝐰 𝐯𝐚𝐥𝐮𝐞 𝐨𝐟 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 𝐢𝐧 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝐚𝐫𝐲 𝐬𝐨 𝐰𝐞 𝐰𝐢𝐥𝐥 𝐬𝐢𝐦𝐩𝐥𝐲 𝐝𝐨 𝐜𝐚𝐥𝐮𝐜𝐚𝐭𝐢𝐨𝐧 𝐨𝐟 𝐟𝐨𝐫𝐦𝐮𝐥𝐚 ( 𝐨𝐥𝐝_𝐰 - 𝐋𝐑 * 𝐝𝐢𝐟𝐟) 𝐨𝐧 𝐩𝐝_𝐛𝐚𝐤𝐜𝐰𝐚𝐫𝐝_𝐰𝐞𝐢𝐠𝐡𝐭𝐬 𝐚𝐧𝐝 𝐬𝐭𝐨𝐫𝐞 𝐭𝐡𝐞 𝐨𝐮𝐭𝐩𝐮𝐭 (𝐧𝐞𝐰 𝐯𝐚𝐥𝐮𝐞 ) 𝐢𝐧 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝐚𝐫𝐲 𝐬𝐨 𝐢𝐧 𝐧𝐞𝐱𝐭 𝐥𝐨𝐨𝐩 𝐮𝐩𝐝𝐚𝐭𝐞𝐝 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝐚𝐫𝐲 𝐰𝐢𝐥𝐥 𝐛𝐞 𝐮𝐬𝐞𝐝 . 𝐓𝐡𝐞𝐧 𝐬𝐚𝐦𝐞 𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐰𝐢𝐥𝐥 𝐫𝐮𝐧 𝐟𝐨𝐫𝐰𝐚𝐫𝐝 𝐚𝐧𝐝 𝐛𝐚𝐜𝐤𝐰𝐚𝐫𝐝 .


----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 
## 𝐏𝐑𝐄𝐃𝐈𝐂𝐓𝐈𝐎𝐍 

𝐧𝐨𝐰 𝐰𝐞  𝐰𝐢𝐥𝐥 𝐚𝐝𝐝 𝐩𝐫𝐞𝐝𝐢𝐜𝐭𝐢𝐨𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐨𝐮𝐭𝐬𝐢𝐝𝐞 𝐭𝐡𝐞 𝐚𝐧𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧  
```yml
def prediction(self,x):
        self.units['0']=x
        for i in range(len(self.units)-1):
            self.units['{}'.format(i)]= np.reshape(self.units['{}'.format(i)],(1,-1)) 
        def forward(units,weights,bias):
            for i in range(len(units)-2):
                units['{}'.format(i+1)] = self.sigmoid(np.dot(units['{}'.format(i)] ,weights['{}'.format(i)] )+bias['{}'.format(i+1)],i)
        forward(self.units,self.weights,self.bias)
        for i in range(len(self.units)-1):
            self.units['{}'.format(i)]= np.reshape(self.units['{}'.format(i)],(-1))     
        return(np.argmax(self.units['{}'.format(len(self.units)-2)]))
```
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# LETS SUM IT ALL 

```yml
import  numpy as np

def get_mnist():
    with np.load(f"C:\\Users\\Ravi\\Desktop\\train\\mnist.npz") as f:
        images, labels = f["x_train"], f["y_train"]
    images = images.astype("float32") / 255
    images = np.reshape(images, (images.shape[0], images.shape[1] * images.shape[2]))
    labels = np.eye(10)[labels]
    return images, labels
input,output= get_mnist()

class ANN:
    import numpy as np,math,copy
    np.random.seed(42)
    def __init__(self):
        self.no_of_units_in_layers=[]
        self.activation_fun=[]
        self.weights={}
        self.units={}
        self.bias={}
    def add(self,unit=0,activation=0):
        self.no_of_units_in_layers.append(int(unit))
        self.activation_fun.append(activation)
    def sigmoid(self,x,i):
        if self.activation_fun[i]=="sigmoid":
            return 1/(1 + self.math.e**(-x))
        elif self.activation_fun[i]=="relu":
            if x<0:
                return 0
            else:
                return x
        elif self.activation_fun[i]=="softplus":
           return self.math.log(1+self.math.e**x,base=self.math.e)
        elif self.activation_fun[i]=="tanh":
            return (self.math.e**x - self.math.e**(-x))/(self.math.e**x + self.math.e**(-x))
        else :
            return x    
    def ann(self,inputs,outputs,epochs,learning_rate=0.1):
       ###### creation of weights 
       self.weights['0']=np.random.uniform(-0.5,0.5,(len(inputs[0]),self.no_of_units_in_layers[0]))
       for i in range(len(self.no_of_units_in_layers)-1):
           self.weights['{}'.format(i+1)]=np.random.uniform(-0.5,0.5,(self.no_of_units_in_layers[i],self.no_of_units_in_layers[i+1]))
       for i in range(len(self.no_of_units_in_layers)):
           self.units['{}'.format(i+1)] = np.zeros(self.no_of_units_in_layers[i])
       for i in range(len(self.units)):
           self.bias['{}'.format(i+1)]=np.zeros(len(self.units['{}'.format(i+1)]))
       for y in range(epochs):
         for e in range(len(outputs)):
            self.units['0']=input[e]
            self.units['real']=outputs[e]
            ######   forward phase 
            self.units['{}'.format(0)]=np.reshape(self.units['{}'.format(0)],(1,-1))
            def forward(units,weights,bias):
                for i in range(len(units)-2):
                    units['{}'.format(i+1)] = self.np.reshape(self.sigmoid(self.np.dot(units['{}'.format(i)] ,weights['{}'.format(i)] )+bias['{}'.format(i+1)],i) ,(-1,))        
            forward(self.units,self.weights,self.bias)
            ########## change two dimentional array in unit_dict in to one dimensional
            self.units['{}'.format(0)]= self.np.reshape(self.units['{}'.format(0)],(-1,))
            ##############  error cost function
            error=(1/len(self.units['real']))* np.sum ((self.units['{}'.format(len(self.units)-2)] - self.units['real'] )**2)
            ######## to check how many data classied correctly in training after how many epochs
            print("    error is  =>  ",error , "   and epochs is   ", y,"  ",e)
            ######## back propagation phase 
            pd_backward_weights=self.copy.deepcopy(self.weights)
            def activ_diff(y):
                if self.activation_fun[y]=="sigmoid":
                    return self.units['{}'.format(y+1)]*(1-self.units['{}'.format(y+1)])
                elif self.activation_fun[y]=="relu":
                    return self.units['{}'.format(y+1)]*(self.units['{}'.format(y+1)]>0)    
                elif self.activation_fun[y]==0:
                    return y       
            def backward(pd_backward_weights):
                dd = (self.units["{}".format(len(self.units)-2)] - self.units["real"]) * activ_diff(len(self.units)-3) 
                self.bias['{}'.format(len(self.units)-2)] += - learning_rate* dd
                pd_backward_weights['{}'.format(len(self.units)-3)] = dd *self.units['{}'.format(len(self.units)-3)].reshape(-1,1)
                dd = dd* self.weights['{}'.format(len(self.units)-3)]
                self.weights['{}'.format(len(self.units)-3)] += - learning_rate* pd_backward_weights['{}'.format(len(self.units)-3)]

                for i in reversed(range(len(self.weights)-1)):
                   dd = np.sum(dd * activ_diff(i).reshape(-1,1),axis=1)
                   self.bias['{}'.format(i+1)] += - learning_rate*dd
                   pd_backward_weights['{}'.format(i)] = dd * self.units['{}'.format(i)].reshape(-1,1)
                   dd = dd * self.weights['{}'.format(i)]
                   self.weights['{}'.format(i)] += - learning_rate* pd_backward_weights['{}'.format(i)]
            backward(pd_backward_weights)
    def prediction(self,x):
        self.units['0']=x
        for i in range(len(self.units)-1):
            self.units['{}'.format(i)]= np.reshape(self.units['{}'.format(i)],(1,-1)) 
        def forward(units,weights,bias):
            for i in range(len(units)-2):
                units['{}'.format(i+1)] = self.sigmoid(np.dot(units['{}'.format(i)] ,weights['{}'.format(i)] )+bias['{}'.format(i+1)],i)
        forward(self.units,self.weights,self.bias)
        for i in range(len(self.units)-1):
            self.units['{}'.format(i)]= np.reshape(self.units['{}'.format(i)],(-1))     
        return(np.argmax(self.units['{}'.format(len(self.units)-2)]))

x=ANN()
x.add(15,"sigmoid")
x.add(10)
x.ann(input,output,5,learning_rate=0.01)    # input and output are taken from mnist dataset .
correct_counter=0
for i in range(5999):
    if x.prediction(input[i+54000]) ==np.argmax(output[i+54000]):
      correct_counter+=1
print("percentage is ",(correct_counter/5999)*100," %")
```

