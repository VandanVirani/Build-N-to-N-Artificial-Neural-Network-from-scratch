## Welcome 

𝐓𝐡𝐢𝐬 𝐫𝐞𝐩𝐨𝐬𝐢𝐭𝐨𝐫𝐲 𝐜𝐨𝐧𝐭𝐚𝐢𝐧𝐬 𝐜𝐨𝐝𝐞 𝐭𝐨 𝐛𝐮𝐢𝐥𝐝 𝐍 𝐭𝐨 𝐍 𝐚𝐫𝐭𝐢𝐟𝐢𝐜𝐢𝐚𝐥 𝐍𝐞𝐮𝐫𝐚𝐥 𝐍𝐞𝐭𝐰𝐨𝐫𝐤 , 𝐍 𝐭𝐨 𝐍 𝐦𝐞𝐚𝐧𝐬 𝐍 𝐧𝐮𝐦𝐛𝐞𝐫 𝐨𝐟 𝐥𝐚𝐲𝐞𝐫𝐬 𝐚𝐧𝐝 𝐮𝐧𝐢𝐭𝐬 . 
𝐖𝐞 𝐚𝐫𝐞 𝐠𝐨𝐢𝐧𝐠 𝐭𝐨 𝐮𝐬𝐞 𝐭𝐡𝐞 𝐜𝐨𝐧𝐜𝐞𝐩𝐭 𝐨𝐟 𝐨𝐨𝐩 𝐞.𝐠 𝐜𝐥𝐚𝐬𝐬 , 𝐢𝐧𝐬𝐭𝐚𝐧𝐜𝐞 . 𝐓𝐡𝐢𝐬 𝐩𝐫𝐨𝐣𝐞𝐜𝐭 𝐫𝐞𝐪𝐮𝐢𝐫𝐞 𝐤𝐧𝐨𝐰𝐥𝐞𝐝𝐠𝐞 𝐚𝐛𝐨𝐮𝐭 𝐨𝐨𝐩 𝐜𝐨𝐧𝐜𝐞𝐩𝐭 , 𝐟𝐨𝐫 𝐥𝐨𝐨𝐩 , 𝐚𝐫𝐫𝐚𝐲𝐬 , 𝐦𝐚𝐭𝐫𝐢𝐱 , 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 , 𝐧𝐮𝐦𝐩𝐲 , 𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝐚𝐫𝐲 . 
𝐖𝐞 𝐰𝐢𝐥𝐥 𝐛𝐮𝐢𝐥𝐝 𝐨𝐮𝐫 𝐨𝐰𝐧 𝐦𝐨𝐝𝐞𝐥 𝐚𝐧𝐝  𝐰𝐞 𝐰𝐢𝐥𝐥 𝐢𝐦𝐩𝐥𝐞𝐦𝐞𝐧𝐭 𝐌𝐍𝐈𝐒𝐓 𝐝𝐚𝐭𝐚𝐬𝐞𝐭 𝐭𝐨 𝐨𝐮𝐫 𝐦𝐨𝐝𝐞𝐥 .

MNIST dataset 
<img src="https://user-images.githubusercontent.com/76767487/148059145-c2b13ff0-ac67-4f79-b170-11de64a3d7a6.png" width=600 height=400 />

## 𝐋𝐄𝐓𝐒 𝐆𝐄𝐓 𝐒𝐓𝐀𝐑𝐓𝐄𝐃 

𝐅𝐢𝐫𝐬𝐭  𝐰𝐞 𝐰𝐢𝐥𝐥 𝐮𝐬𝐞 𝐭𝐡𝐢𝐬 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧  𝐭𝐨 𝐠𝐞𝐭 𝐭𝐡𝐞 𝐢𝐧𝐩𝐮𝐭 𝐚𝐧𝐝 𝐨𝐮𝐩𝐮𝐭 𝐟𝐫𝐨𝐦 𝐦𝐧𝐢𝐬𝐭 𝐝𝐚𝐭𝐚 . 𝐓𝐡𝐞 𝐌𝐍𝐈𝐒𝐓 𝐝𝐚𝐭𝐚𝐛𝐚𝐬𝐞 (𝐌𝐨𝐝𝐢𝐟𝐢𝐞𝐝 𝐍𝐚𝐭𝐢𝐨𝐧𝐚𝐥 𝐈𝐧𝐬𝐭𝐢𝐭𝐮𝐭𝐞 𝐨𝐟 𝐒𝐭𝐚𝐧𝐝𝐚𝐫𝐝𝐬 𝐚𝐧𝐝 𝐓𝐞𝐜𝐡𝐧𝐨𝐥𝐨𝐠𝐲 𝐝𝐚𝐭𝐚𝐛𝐚𝐬𝐞[𝟏]) 𝐢𝐬 𝐚 𝐥𝐚𝐫𝐠𝐞 𝐝𝐚𝐭𝐚𝐛𝐚𝐬𝐞 𝐨𝐟 𝐡𝐚𝐧𝐝𝐰𝐫𝐢𝐭𝐭𝐞𝐧 𝐝𝐢𝐠𝐢𝐭𝐬 𝐭𝐡𝐚𝐭 𝐢𝐬 𝐜𝐨𝐦𝐦𝐨𝐧𝐥𝐲 𝐮𝐬𝐞𝐝 𝐟𝐨𝐫 𝐭𝐫𝐚𝐢𝐧𝐢𝐧𝐠 𝐯𝐚𝐫𝐢𝐨𝐮𝐬 𝐢𝐦𝐚𝐠𝐞 𝐩𝐫𝐨𝐜𝐞𝐬𝐬𝐢𝐧𝐠 𝐬𝐲𝐬𝐭𝐞𝐦𝐬 . 𝐓𝐡𝐞 𝐌𝐍𝐈𝐒𝐓 𝐝𝐚𝐭𝐚𝐛𝐚𝐬𝐞 𝐜𝐨𝐧𝐭𝐚𝐢𝐧𝐬 𝟔𝟎,𝟎𝟎𝟎 𝐭𝐫𝐚𝐢𝐧𝐢𝐧𝐠 𝐢𝐦𝐚𝐠𝐞𝐬 . 
𝐁𝐞𝐜𝐚𝐮𝐬𝐞 𝐭𝐡𝐞𝐫𝐞 𝐚𝐫𝐞 𝐭𝐨𝐭𝐚𝐥 𝟏𝟎 𝐝𝐢𝐠𝐢𝐭𝐬 𝟎,𝟏,𝟐,𝟑,𝟒,𝟓,𝟔,𝟕,𝟖,𝟗 𝐭𝐡𝐞𝐫𝐞 𝐰𝐢𝐥𝐥 𝐛𝐞 𝟏𝟎 𝐜𝐥𝐚𝐬𝐬 𝐢𝐧 𝐥𝐚𝐬𝐭 𝐥𝐚𝐲𝐞𝐫 .
```yml 
import  numpy as np

def get_mnist():
    with np.load(f"D:/python programing/test/ann/mnist.npz") as f:
        images, labels = f["x_train"], f["y_train"]
    images = images.astype("float32") / 255
    images = np.reshape(images, (images.shape[0], images.shape[1] * images.shape[2]))
    labels = np.eye(10)[labels]
    return images, labels
input,output= get_mnist()
print(input,output)
print(input.shape,output.shape)
```
<img src="https://user-images.githubusercontent.com/76767487/148325194-287d827c-a3dd-4fda-9038-cb1349feba94.jpeg" width=900 height=600 />

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
𝐰𝐞 𝐰𝐢𝐥𝐥 𝐜𝐫𝐞𝐚𝐭𝐞 𝐜𝐥𝐚𝐬𝐬 𝐰𝐡𝐢𝐜𝐡 𝐰𝐢𝐥𝐥 𝐡𝐞𝐥𝐩 𝐭𝐨 𝐠𝐞𝐭 𝐢𝐧𝐬𝐭𝐚𝐧𝐜𝐞 , 𝐚𝐧𝐝 𝐢𝐧𝐬𝐢𝐝𝐞 𝐭𝐡𝐚𝐭 𝐟𝐢𝐫𝐬𝐭 𝐰𝐞 𝐰𝐢𝐥𝐥 𝐢𝐧𝐢𝐭𝐢𝐚𝐥𝐢𝐳𝐞 𝐥𝐢𝐬𝐭 𝐚𝐧𝐝 𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝐚𝐫𝐲 , 𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝐚𝐫𝐲 𝐢𝐬 𝐦𝐚𝐢𝐧 𝐬𝐭𝐨𝐫𝐚𝐠𝐞 𝐭𝐨 𝐬𝐭𝐨𝐫𝐞 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 , 𝐮𝐧𝐢𝐭𝐬 .
```yml
 class ANN : 
    def __init__(self):
        self.no_of_units_in_layers=[]
        self.activation_fun=[]
        self.weights={}
        self.units={}
        self.bias={}
```
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
𝐁𝐞𝐥𝐨𝐰  ,  𝐰𝐞 𝐡𝐚𝐯𝐞 𝐦𝐚𝐝𝐞 𝐢𝐧𝐬𝐭𝐚𝐧𝐜𝐞 𝐨𝐟 𝐀𝐍𝐍 𝐜𝐥𝐚𝐬𝐬 𝐰𝐡𝐢𝐜𝐡 𝐢𝐬 𝐱 , 𝐡𝐞𝐫𝐞 𝐰𝐞 𝐡𝐚𝐯𝐞 𝐦𝐚𝐝𝐞 𝐚 𝐚𝐝𝐝 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐰𝐡𝐢𝐜𝐡 𝐰𝐢𝐥𝐥 𝐜𝐫𝐞𝐚𝐭𝐞 𝐚 𝐥𝐚𝐲𝐞𝐫 , 𝐢𝐭 𝐫𝐞𝐪𝐮𝐢𝐫𝐞 𝐧𝐨 𝐨𝐟 𝐮𝐧𝐢𝐭𝐬 𝐚𝐧𝐝 𝐚𝐧 𝐚𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐚𝐬𝐬𝐨𝐜𝐢𝐚𝐭𝐞𝐝 𝐭𝐨 𝐭𝐡𝐚𝐭 𝐥𝐚𝐲𝐞𝐫 . 𝐰𝐞 𝐡𝐚𝐯𝐞 𝐚𝐝𝐝𝐞𝐝 𝐭𝐡𝐞 𝐯𝐚𝐥𝐮𝐞 𝐨𝐟 𝐮𝐧𝐢𝐭𝐬 𝐚𝐧𝐝 𝐚𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧 𝐭𝐨 𝐥𝐢𝐬𝐭 . 𝐡𝐞𝐫𝐞 𝐰𝐞 𝐡𝐚𝐯𝐞 𝐚𝐝𝐝 𝐭𝐰𝐨 𝐭𝐢𝐦𝐞 𝐚𝐝𝐝 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐦𝐞𝐚𝐧𝐬 𝐰𝐞 𝐡𝐚𝐯𝐞 𝐭𝐰𝐨 𝐥𝐚𝐲𝐞𝐫𝐬 𝐨𝐧𝐞 𝐰𝐢𝐭𝐡 𝟑 𝐮𝐧𝐢𝐭𝐬 𝐰𝐢𝐭𝐡 𝐬𝐢𝐠𝐦𝐨𝐢𝐝 𝐚𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧 𝐚𝐧𝐝 𝐬𝐞𝐜𝐨𝐧𝐝 𝐢𝐬 𝟐 𝐮𝐧𝐢𝐭𝐬 . 𝐥𝐚𝐬𝐭 𝐭𝐢𝐦𝐞 𝐰𝐞 𝐜𝐚𝐥𝐥 𝐚𝐝𝐝 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐢𝐬 𝐨𝐮𝐫 𝐨𝐮𝐭𝐩𝐮𝐭 𝐥𝐚𝐲𝐞𝐫 , 𝐰𝐞 𝐡𝐚𝐯𝐞 𝟐 𝐮𝐧𝐢𝐭𝐬 𝐢𝐧 𝐨𝐮𝐭𝐩𝐮𝐭 𝐥𝐚𝐲𝐞𝐫 𝐦𝐞𝐚𝐧𝐬 𝐰𝐞 𝐡𝐚𝐯𝐞 𝟐 𝐜𝐥𝐚𝐬𝐬 (:. 𝐥𝐢𝐤𝐞 𝐜𝐚𝐭,𝐝𝐨𝐠 ).
```yml
class ANN:
    def __init__(self):
        self.no_of_units_in_layers=[]
        self.activation_fun=[]
        self.weights={}
        self.units={}
        self.bias={}
    
    def add(self,unit,activation=0):                 # default value of activation is 0
        self.no_of_units_in_layers.append(int(unit))   # append the information in no_of_units_in_layers list
        self.activation_fun.append(activation)         # append the information in activation_fun list

x=ANN()
x.add(3,"sigmoid")
x.add(10)   # because 10 class are there 
```        
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
𝐍𝐨𝐰 𝐢𝐭𝐬 𝐭𝐢𝐦𝐞 𝐭𝐨 𝐜𝐫𝐞𝐚𝐭𝐞 𝐚𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 , 𝐢𝐭 𝐭𝐚𝐤𝐞𝐬 𝐭𝐰𝐨 𝐚𝐫𝐠𝐮𝐦𝐞𝐧𝐭𝐬 𝐟𝐢𝐫𝐬𝐭 𝐢𝐬 𝐚𝐫𝐫𝐚𝐲 𝐚𝐧𝐝 𝐬𝐞𝐜𝐨𝐧𝐝 𝐢𝐬 𝐚 𝐧𝐮𝐦𝐛𝐞𝐫 𝐢𝐭 𝐭𝐞𝐥𝐥𝐬 𝐰𝐡𝐚𝐭 𝐥𝐚𝐲𝐞𝐫 𝐧𝐞𝐞𝐝 𝐭𝐨 𝐮𝐬𝐞 𝐰𝐡𝐢𝐜𝐡 𝐚𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 . 
𝐬𝐢𝐠𝐦𝐨𝐢𝐝 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 : 𝐜𝐨𝐧𝐯𝐞𝐫𝐭𝐬 +𝐢𝐯𝐞 𝐢𝐧𝐭𝐞𝐠𝐞𝐫 𝐭𝐨 𝐚  𝐧𝐮𝐦𝐛𝐞𝐫 𝐢𝐧 𝐛𝐞𝐭𝐰𝐞𝐞𝐧 𝟎 𝐭𝐨 𝟏 . 𝐟𝐨𝐫𝐦𝐮𝐥𝐚 𝐢𝐬 𝟏 / (𝟏 + 𝐞**(-𝐱) ) , 𝐰𝐞 𝐡𝐚𝐯𝐞 𝐮𝐬𝐞 𝐦𝐚𝐭𝐡 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐭𝐨 𝐮𝐬𝐞 𝐯𝐚𝐥𝐮𝐞 𝐨𝐟 𝐞 . 
𝐫𝐞𝐥𝐮 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧    : 𝐢𝐟 𝐯𝐚𝐥𝐮𝐞 𝐥𝐞𝐬𝐬 𝐭𝐡𝐚𝐧 𝟎 𝐭𝐡𝐚𝐧 𝐢𝐭 𝐰𝐢𝐥𝐥 𝐫𝐞𝐭𝐮𝐫𝐧 𝟎 𝐨𝐭𝐡𝐞𝐫𝐰𝐢𝐬𝐞 𝐢𝐭 𝐰𝐢𝐥𝐥 𝐫𝐞𝐭𝐮𝐫𝐧 𝐭𝐡𝐞 𝐨𝐫𝐢𝐠𝐢𝐧𝐚𝐥 𝐯𝐚𝐥𝐮𝐞 .
```yml
class ANN:
    import math
    def __init__(self):
        self.no_of_units_in_layers=[]
        self.activation_fun=[]
        self.weights={}
        self.units={}
        self.bias={}
    
    def add(self,unit=0,activation=0):  # default value of activation is 0
        self.no_of_units_in_layers.append(int(unit))
        self.activation_fun.append(activation)
    def activation_function(self,x,i):
        if self.activation_fun[i]=="sigmoid":
            return 1/(1 + self.math.e**(-x))
        elif self.activation_fun[i]=="relu":
            return x*(x>0)    
x=ANN()
x.add(3,"sigmoid")
x.add(2)
```     
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
𝐍𝐨𝐰 𝐢𝐭𝐬 𝐭𝐢𝐦𝐞 𝐭𝐨 𝐭𝐚𝐤𝐞 𝐢𝐧𝐩𝐮𝐭 , 𝐨𝐮𝐭𝐩𝐮𝐭 , 𝐥𝐞𝐚𝐫𝐧𝐢𝐧𝐠 𝐫𝐚𝐭𝐞 , 𝐞𝐩𝐨𝐜𝐡𝐬 𝐭𝐨 𝐨𝐮𝐫 𝐦𝐨𝐝𝐞𝐥 .𝐭𝐨 𝐝𝐨 𝐭𝐡𝐚𝐭 𝐰𝐞 𝐡𝐚𝐯𝐞 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐜𝐚𝐥𝐥𝐞𝐝 𝐚𝐧𝐧. 𝐚𝐥𝐥 𝐭𝐡𝐞 𝐭𝐡𝐢𝐧𝐠𝐬 𝐡𝐚𝐩𝐩𝐞𝐧 𝐢𝐧𝐬𝐢𝐝𝐞 𝐭𝐡𝐞 𝐚𝐧𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 . 𝐰𝐞 𝐮𝐬𝐞 𝐧𝐮𝐦𝐩𝐲 𝐥𝐢𝐛𝐫𝐚𝐫𝐲 𝐟𝐨𝐫 𝐜𝐫𝐞𝐚𝐭𝐢𝐨𝐧 𝐨𝐟 𝐚𝐫𝐫𝐚𝐲 , 𝐠𝐞𝐭𝐭𝐢𝐧𝐠 𝐫𝐚𝐧𝐝𝐨𝐦 𝐯𝐚𝐥𝐮𝐞𝐬 . 𝐖𝐞 𝐡𝐚𝐯𝐞 𝐜𝐚𝐥𝐥 𝐚𝐧𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐚𝐭 𝐥𝐚𝐬𝐭 𝐢𝐧 𝐜𝐨𝐝𝐞. 𝐢𝐧𝐩𝐮𝐭 𝐢𝐬 𝐚𝐧 𝐚𝐫𝐫𝐚𝐲 𝐟𝐨𝐫 𝐞𝐱𝐚𝐩𝐥𝐞 𝐌𝐍𝐈𝐒𝐓 𝐝𝐚𝐭𝐚𝐬𝐞𝐭 𝐜𝐨𝐧𝐭𝐚𝐢𝐧𝐬 
𝟏) 𝐜𝐫𝐞𝐚𝐭𝐢𝐨𝐧 𝐨𝐟 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 : 𝐢𝐧𝐢𝐭𝐢𝐚𝐥𝐥𝐲 𝐭𝐡𝐞 𝐯𝐚𝐥𝐮𝐞 𝐨𝐟 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 𝐢𝐬 𝐫𝐚𝐧𝐝𝐨𝐦 𝐚𝐧𝐝 𝐠𝐞𝐭 𝐜𝐡𝐚𝐧𝐠𝐞 𝐛𝐲 𝐛𝐚𝐜𝐤𝐩𝐫𝐨𝐩𝐚𝐠𝐚𝐭𝐢𝐨𝐧 . 𝐭𝐨 𝐬𝐭𝐨𝐫𝐞 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 𝐰𝐞 𝐮𝐬𝐞 𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝐚𝐫𝐲

```yml

class ANN:
    import math,numpy as np
    def __init__(self):
        self.no_of_units_in_layers=[]
        self.activation_fun=[]
        self.weights={}
        self.units={}
        self.bias={}
    
    def add(self,unit=0,activation=0):  # default value of activation is 0
        self.no_of_units_in_layers.append(int(unit))
        self.activation_fun.append(activation)
    def activation_function(self,x,i):
        if self.activation_fun[i]=="sigmoid":
            return 1/(1 + self.math.e**(-x))
        elif self.activation_fun[i]=="relu":
            return x*(x>0)    
    def ann(self,inputs,outputs,epochs=10,learning_rate=0.1):  # default value  of learning rate is 0.1 and epochs is 10 
       ###### creation of weights 
       self.weights['0']=np.random.uniform(-0.5,0.5,(len(inputs[0]),self.no_of_units_in_layers[0]))
       for i in range(len(self.no_of_units_in_layers)-1):
           self.weights['{}'.format(i+1)]=np.random.uniform(-0.5,0.5,(self.no_of_units_in_layers[i],self.no_of_units_in_layers[i+1]))
       print("weights : ",self.weights)    
       ###### initializing the units 
       for i in range(len(self.no_of_units_in_layers)):
           self.units['{}'.format(i+1)] = np.zeros(self.no_of_units_in_layers[i])
       print("units : ",self.units)        
       for i in range(len(self.units)):
           self.bias['{}'.format(i+1)]=np.zeros(len(self.units['{}'.format(i+1)]))
       print("bias : ",self.bias)    
       
x=ANN()
x.add(3,"sigmoid")
x.add(10)
x.ann(input,output,500,learning_rate=0.01)    # input and output are taken from mnist dataset . 
```  

<img src="https://user-images.githubusercontent.com/76767487/148326714-1543568b-bd7d-4b36-8e71-06b12e3107c9.jpg" width=900 height=230 />

𝐍𝐨𝐰 𝐰𝐞 𝐰𝐢𝐥𝐥 𝐛𝐞 𝐭𝐡𝐢𝐧𝐤𝐢𝐧𝐠 𝐡𝐨𝐰 𝐢 𝐡𝐚𝐯𝐞 𝐜𝐫𝐞𝐚𝐭𝐞𝐝 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 . 𝐬𝐞𝐞 𝐛𝐞𝐥𝐨𝐰 𝐢𝐦𝐚𝐠𝐞



![WhatsApp Image 2022-01-06 at 9 59 03 AM](https://user-images.githubusercontent.com/76767487/148328318-cb6342a4-db48-4b8a-9962-e4ba904fb66e.jpeg)

𝐈𝐧𝐢𝐭𝐢𝐚𝐥𝐥𝐲 𝐭𝐡𝐞 𝐮𝐧𝐢𝐭𝐬 𝐯𝐚𝐥𝐮𝐞 𝐢𝐧 𝐮𝐧𝐢𝐭𝐬 𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝐚𝐫𝐲 𝐢𝐬 𝟎 , 𝐢𝐭 𝐰𝐢𝐥𝐥 𝐜𝐡𝐚𝐧𝐠𝐞𝐝 𝐝𝐮𝐫𝐢𝐧𝐠 𝐟𝐨𝐫𝐰𝐚𝐫𝐝 𝐩𝐫𝐨𝐩𝐚𝐠𝐚𝐭𝐢𝐨𝐧 .

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 

## FORWARD PROPAGATION  


```yml
def ann(self,inputs,outputs,epochs=10,learning_rate=0.1):  # default value  of learning rate is 0.1 and epochs is 10 
       ###### creation of weights 
       self.weights['0']=np.random.uniform(-0.5,0.5,(len(inputs[0]),self.no_of_units_in_layers[0]))
       for i in range(len(self.no_of_units_in_layers)-1):
           self.weights['{}'.format(i+1)]=np.random.uniform(-0.5,0.5,(self.no_of_units_in_layers[i],self.no_of_units_in_layers[i+1]))
       print("weights : ",self.weights)    
       ###### initializing the units 
       for i in range(len(self.no_of_units_in_layers)):
           self.units['{}'.format(i+1)] = np.zeros(self.no_of_units_in_layers[i])
       print("units : ",self.units)        
       for i in range(len(self.units)):
           self.bias['{}'.format(i+1)]=np.zeros(len(self.units['{}'.format(i+1)]))
       print("bias : ",self.bias)    
       
       #-------------------------------------------------------->
       for y in range(epochs):
         for e in range(len(outputs)):
            self.units['0']=input[e]
            self.units['real']=outputs[e]
            
            ######   forward phase 
            ########## change one  dimentional array in unit_dict in to two dimensional
            self.units['{}'.format(0)]=np.reshape(self.units['{}'.format(0)],(1,-1))   # because we want to calculate matrix multiplication 
            def forward(units,weights,bias):
                for i in range(len(units)-2):
                    units['{}'.format(i+1)] = self.np.reshape(self.sigmoid(self.np.dot(units['{}'.format(i)] ,weights['{}'.format(i)] )+bias['{}'.format(i+1)],i) ,(-1,))        
            forward(self.units,self.weights,self.bias)
            ########## change two dimentional array in unit_dict in to one dimensional
            self.units['{}'.format(0)]= self.np.reshape(self.units['{}'.format(0)],(-1,))
```  

here , the creation of weights , bias , initializing units will run only one time but the forward propagation , backwackprapagation will run more than one for training so we wil write entire forward and backward code in for loop , The regularisation of the for loop will be done by epochs , if epochs is 1 the our code will analyse the entire dataset once , if epochs is 10 then it will scan 10 times . if you don't understand then try to run the parts of the code . 


----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 
## LOSS 

we are adding loss calculation , loss function can be varies , here we used squared loss in ann function after forward propagation .  

```         
            error = (1/len(self.units['real']))* np.sum ((self.units['{}'.format(len(self.units)-2)] - self.units['real'] )**2)
            ######## to check how many data classied correctly in training after how many epochs
            print("    error is  =>  ",error , "   and epochs is   ", y,"  ",e)
```            

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 
## BACKPROPAGATION 

```
   def ann(self,inputs,outputs,epochs,learning_rate=0.1):
       ###### creation of weights 
       self.weights['0']=np.random.uniform(-0.5,0.5,(len(inputs[0]),self.no_of_units_in_layers[0]))
       for i in range(len(self.no_of_units_in_layers)-1):
           self.weights['{}'.format(i+1)]=np.random.uniform(-0.5,0.5,(self.no_of_units_in_layers[i],self.no_of_units_in_layers[i+1]))
       for i in range(len(self.no_of_units_in_layers)):
           self.units['{}'.format(i+1)] = np.zeros(self.no_of_units_in_layers[i])
       for i in range(len(self.units)):
           self.bias['{}'.format(i+1)]=np.zeros(len(self.units['{}'.format(i+1)]))
       for y in range(epochs):
         for e in range(len(outputs)):
            self.units['0']=input[e]
            self.units['real']=outputs[e]
            ######   forward phase 
            self.units['{}'.format(0)]=np.reshape(self.units['{}'.format(0)],(1,-1))
            def forward(units,weights,bias):
                for i in range(len(units)-2):
                    units['{}'.format(i+1)] = self.np.reshape(self.sigmoid(self.np.dot(units['{}'.format(i)] ,weights['{}'.format(i)] )+bias['{}'.format(i+1)],i) ,(-1,))        
            forward(self.units,self.weights,self.bias)
            ########## change two dimentional array in unit_dict in to one dimensional
            self.units['{}'.format(0)]= self.np.reshape(self.units['{}'.format(0)],(-1,))
            
            
            ##############  error cost function
            error=(1/len(self.units['real']))* np.sum ((self.units['{}'.format(len(self.units)-2)] - self.units['real'] )**2)
            ######## to check how many data classied correctly in training after how many epochs
            print("    error is  =>  ",error , "   and epochs is   ", y,"  ",e)
            
            
            
            ######## back propagation phase 
            pd_backward_weights=self.copy.deepcopy(self.weights)
            def activ_diff(y):
                if self.activation_fun[y]=="sigmoid":
                    return self.units['{}'.format(y+1)]*(1-self.units['{}'.format(y+1)])
                elif self.activation_fun[y]=="relu":
                    return self.units['{}'.format(y+1)]*(self.units['{}'.format(y+1)]>0)    
            def backward(pd_backward_weights):
                dd = (self.units["{}".format(len(self.units)-2)] - self.units["real"]) * activ_diff(len(self.units)-3) 
                self.bias['{}'.format(len(self.units)-2)] += - learning_rate* dd
                pd_backward_weights['{}'.format(len(self.units)-3)] = dd *self.units['{}'.format(len(self.units)-3)].reshape(-1,1)
                dd = dd* self.weights['{}'.format(len(self.units)-3)]
                self.weights['{}'.format(len(self.units)-3)] += - learning_rate* pd_backward_weights['{}'.format(len(self.units)-3)]

                for i in reversed(range(len(self.weights)-1)):
                   dd = np.sum(dd * activ_diff(i).reshape(-1,1),axis=1)
                   self.bias['{}'.format(i+1)] += - learning_rate*dd
                   pd_backward_weights['{}'.format(i)] = dd * self.units['{}'.format(i)].reshape(-1,1)
                   dd = dd * self.weights['{}'.format(i)]
                   self.weights['{}'.format(i)] += - learning_rate* pd_backward_weights['{}'.format(i)]
            backward(pd_backward_weights)
```
